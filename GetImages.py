import requests
import time
import os
from urllib.parse import urlparse
import hashlib
import json

# ===================== åŸºç¡€é…ç½®ï¼ˆæŒ‰éœ€ä¿®æ”¹ï¼‰=====================
# 1. ä½ çš„Cookieï¼ˆä»æµè§ˆå™¨å¤åˆ¶ï¼Œå…³é”®ï¼ï¼‰
cookies = {
    'BIDUPSID': 'C4210F4FCE75E84D7CBA7F6D7D2659D3',
    'PSTM': '1761921363',
    'BAIDUID': 'B610A82293895758157FE5971ACA8A1E:FG=1',
    'H_PS_PSSID': '60272_63140_64004_64979_65250_65313_65361_65588_65604_65759_65778_65789_65843_65852_65942_65953_65960_65971_65999_66076_66099_66111_65636_65866',
    'BDUSS_BFESS': 'FNeE5JbmtMcTBQMDQxSXJpaDJJeEFDUzBCMlp-d0lhbUN1flhlS01mZkZZaXhwSUFBQUFBJCQAAAAAAQAAAAEAAAAdzH94wuTEu9PAsrvC5MS7AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAMXVBGnF1QRpZ0',
    'BAIDUID_BFESS': 'B610A82293895758157FE5971ACA8A1E:FG=1',
    'ZFY': 'P1g9sAm:AIqf43uLXGWYX3mblTC0WbgD95aBXu18:BOBM:C',
    'BDRCVFR[BIVAaPonX6T]': '-_EV5wtlMr0mh-8uz4WUvY',
    'BA_HECTOR': '8h2gal0ka18l04a5a52h8ha0alaha31kgga6h24',
    'PSINO': '3',
    'delPer': '0',
    'BDORZ': 'FFFB88E999055A3F8A630C64834BD6D0',
    'H_WISE_SIDS': '64979_65250_65313_65361_65604_65778_65789_65852_65942_65953_65999_66076_66099_66111_65636',
}

# 2. è¯·æ±‚å¤´ï¼ˆæ¨¡æ‹Ÿæµè§ˆå™¨ï¼Œæ— éœ€ä¿®æ”¹ï¼‰
headers = {
    'Accept': 'application/json, text/plain, */*',
    'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8,en-GB;q=0.7,en-US;q=0.6',
    'Connection': 'keep-alive',
    'Referer': 'https://image.baidu.com/search/index?tn=baiduimage&ps=1&ct=201326592&lm=-1&cl=2&nc=1&ie=utf-8&lid=b88e601d000412cf&dyTabStr=MTIsMCwzLDEsMiwxMyw3LDYsNSw5&word=%E8%8D%89%E4%B9%A6',
    'Sec-Fetch-Dest': 'empty',
    'Sec-Fetch-Mode': 'cors',
    'Sec-Fetch-Site': 'same-origin',
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/142.0.0.0 Safari/537.36 Edg/142.0.0.0',
    'sec-ch-ua': '"Chromium";v="142", "Microsoft Edge";v="142", "Not_A Brand";v="99"',
    'sec-ch-ua-mobile': '?0',
    'sec-ch-ua-platform': '"Windows"',
    # 'Cookie': 'BIDUPSID=C4210F4FCE75E84D7CBA7F6D7D2659D3; PSTM=1761921363; BAIDUID=B610A82293895758157FE5971ACA8A1E:FG=1; H_PS_PSSID=60272_63140_64004_64979_65250_65313_65361_65588_65604_65759_65778_65789_65843_65852_65942_65953_65960_65971_65999_66076_66099_66111_65636_65866; BDUSS_BFESS=FNeE5JbmtMcTBQMDQxSXJpaDJJeEFDUzBCMlp-d0lhbUN1flhlS01mZkZZaXhwSUFBQUFBJCQAAAAAAQAAAAEAAAAdzH94wuTEu9PAsrvC5MS7AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAMXVBGnF1QRpZ0; BAIDUID_BFESS=B610A82293895758157FE5971ACA8A1E:FG=1; ZFY=P1g9sAm:AIqf43uLXGWYX3mblTC0WbgD95aBXu18:BOBM:C; BDRCVFR[BIVAaPonX6T]=-_EV5wtlMr0mh-8uz4WUvY; BA_HECTOR=8h2gal0ka18l04a5a52h8ha0alaha31kgga6h24; PSINO=3; delPer=0; BDORZ=FFFB88E999055A3F8A630C64834BD6D0; H_WISE_SIDS=64979_65250_65313_65361_65604_65778_65789_65852_65942_65953_65999_66076_66099_66111_65636',
}
# 3. çˆ¬å–é…ç½®
KEYWORD = 'è‰ä¹¦'  # æœç´¢å…³é”®è¯ï¼ˆå¯ä¿®æ”¹ä¸ºå…¶ä»–å†…å®¹ï¼‰
SAVE_DIR = 'caoshu'  # ä¿å­˜å›¾ç‰‡çš„æ–‡ä»¶å¤¹å
MAX_PAGES = 10  # çˆ¬å–é¡µæ•°ï¼ˆ1é¡µ=30å¼ ï¼Œ5é¡µ=150å¼ ï¼Œå»ºè®®ä¸è¶…è¿‡10é¡µï¼‰
DELAY = 2  # æ¯é¡µè¯·æ±‚é—´éš”ï¼ˆç§’ï¼Œå»ºè®®1-3ç§’ï¼Œé˜²åçˆ¬ï¼‰
RECORD_FILE = 'last_page_cao.json'  # ä¿å­˜ä¸Šæ¬¡è¿›åº¦çš„æ–‡ä»¶

# è¯»å–ä¸Šæ¬¡çˆ¬å–è¿›åº¦
if os.path.exists(RECORD_FILE):
    with open(RECORD_FILE, 'r', encoding='utf-8') as f:
        record = json.load(f)
        START_PAGE = record.get(KEYWORD, 0) + 1
else:
    record = {}
    START_PAGE = 1

END_PAGE = START_PAGE + MAX_PAGES - 1
print(f"ğŸ“˜ æœ¬æ¬¡å°†çˆ¬å–å…³é”®è¯ã€Œ{KEYWORD}ã€çš„ç¬¬ {START_PAGE} é¡µåˆ°ç¬¬ {END_PAGE} é¡µã€‚")

# 4. æ¥å£åŸºç¡€å‚æ•°ï¼ˆæ— éœ€ä¿®æ”¹ï¼‰
base_params = {
    'tn': 'resultjson_com',
    'word': KEYWORD,
    'ie': 'utf-8',
    'fp': 'result',
    'rn': '30',  # æ¯é¡µå›ºå®š30å¼ ï¼ˆç™¾åº¦æ¥å£æœ€å¤§é™åˆ¶ï¼‰
    'nojc': '0',
    'gsm': '3c',
    'newReq': '1',
}

# ===================== å·¥å…·å‡½æ•° =====================
def create_save_dir():
    if not os.path.exists(SAVE_DIR):
        os.makedirs(SAVE_DIR)
    print(f'ğŸ“ å›¾ç‰‡å°†ä¿å­˜åˆ°ï¼š{os.path.abspath(SAVE_DIR)}')

def download_img(img_url, save_path):
    """ä¸‹è½½å•å¼ å›¾ç‰‡"""
    try:
        resp = requests.get(img_url, headers=headers, cookies=cookies, timeout=10, stream=True)
        resp.raise_for_status()
        with open(save_path, 'wb') as f:
            for chunk in resp.iter_content(1024):
                f.write(chunk)
        return True
    except Exception as e:
        print(f'âŒ ä¸‹è½½å¤±è´¥ {img_url}ï¼š{str(e)[:50]}')
        return False

def get_img_filename(img_url):
    """ç”Ÿæˆå”¯ä¸€æ–‡ä»¶å"""
    url_hash = hashlib.md5(img_url.encode()).hexdigest()
    ext = os.path.splitext(urlparse(img_url).path)[1]
    ext = ext[:5] if ext else '.jpg'
    return f'{url_hash}{ext}'

# ===================== ä¸»çˆ¬å–é€»è¾‘ =====================
def main():
    create_save_dir()
    all_img_urls = set()
    success_count = fail_count = 0

    for page in range(START_PAGE, END_PAGE + 1):
        pn = (page - 1) * 30
        base_params['pn'] = pn

        try:
            time.sleep(DELAY)
            print(f'\nğŸ” æ­£åœ¨çˆ¬å–ç¬¬ {page} é¡µ...')
            response = requests.get(
                'https://image.baidu.com/search/acjson',
                params=base_params, cookies=cookies, headers=headers, timeout=10
            )
            data = response.json()
            img_data_list = data['data']['images']

            current_urls = []
            for item in img_data_list:
                if isinstance(item, dict):
                    url = item.get('objurl')
                    if url and url not in all_img_urls:
                        current_urls.append(url)
                        all_img_urls.add(url)

            print(f'ğŸ“¸ å…±æ‰¾åˆ° {len(current_urls)} å¼ å›¾ç‰‡')
            for img_url in current_urls:
                filename = get_img_filename(img_url)
                save_path = os.path.join(SAVE_DIR, filename)
                if download_img(img_url, save_path):
                    success_count += 1
                else:
                    fail_count += 1

            # âœ… æ¯çˆ¬å®Œä¸€é¡µå°±ä¿å­˜å½“å‰è¿›åº¦
            record[KEYWORD] = page
            with open(RECORD_FILE, 'w', encoding='utf-8') as f:
                json.dump(record, f, ensure_ascii=False, indent=2)

        except Exception as e:
            print(f'âš ï¸ ç¬¬ {page} é¡µå‡ºé”™ï¼š{str(e)}')
            continue

    print('\n' + '='*60)
    print(f'âœ… æœ¬æ¬¡å…±çˆ¬å– {START_PAGE}-{END_PAGE} é¡µ')
    print(f'ğŸ“¥ æˆåŠŸä¸‹è½½ï¼š{success_count} å¼  | âŒ å¤±è´¥ï¼š{fail_count} å¼ ')
    print(f'ğŸ“‚ å›¾ç‰‡ä¿å­˜è·¯å¾„ï¼š{os.path.abspath(SAVE_DIR)}')

if __name__ == '__main__':
    main()